{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "conda create -n tf_env python=3.10 pip -y\n",
    "conda activate tf_env\n",
    "pip install tensorflow==2.17.1 keras==3.5.0 numpy==1.26.4 scipy==1.13.1 seaborn==0.13.2 tensorflow-model-optimization==0.8.0 scikit-plot==0.3.7 tf_keras==2.17.0\n",
    "python3 -m pip install tensorflow[and-cuda]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.1\n",
      "NumPy version: 1.26.4\n",
      "SciPy version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "import time \n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"SciPy version: {scipy.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU está conectada\n",
      "Nome da GPU: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU está conectada\")\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        print(f\"Nome da GPU: {gpu.name}\")\n",
    "else:\n",
    "    print(\"Não conectado a uma GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, average_precision_score\n",
    "from scipy.stats import ks_2samp\n",
    "#import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import interp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def extract_final_losses(history):\n",
    "    \"\"\"Função para extrair o melhor loss de treino e validação.\n",
    "\n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "\n",
    "    Retorno:\n",
    "    Dicionário contendo o melhor loss de treino e de validação baseado\n",
    "    no menor loss de validação.\n",
    "    \"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    idx_min_val_loss = np.argmin(val_loss)\n",
    "    return {'train_loss': train_loss[idx_min_val_loss], 'val_loss': val_loss[idx_min_val_loss]}\n",
    "\n",
    "def plot_training_error_curves(history):\n",
    "    \"\"\"Função para plotar as curvas de erro do treinamento da rede neural.\n",
    "\n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "\n",
    "    Retorno:\n",
    "    A função gera o gráfico do treino da rede e retorna None.\n",
    "    \"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_loss, label='Train')\n",
    "    ax.plot(val_loss, label='Validation')\n",
    "    ax.set(title='Training and Validation Error Curves', xlabel='Epochs', ylabel='Loss (MSE)')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def compute_performance_metrics(y, y_pred_class, y_pred_scores=None):\n",
    "    accuracy = accuracy_score(y, y_pred_class)\n",
    "    recall = recall_score(y, y_pred_class)\n",
    "    precision = precision_score(y, y_pred_class)\n",
    "    f1 = f1_score(y, y_pred_class)\n",
    "    performance_metrics = (accuracy, recall, precision, f1)\n",
    "    if y_pred_scores is not None:\n",
    "        # skplt.metrics.plot_ks_statistic(y, y_pred_scores)\n",
    "        # plt.savefig(\"ks_plot.png\")\n",
    "        # plt.show()\n",
    "        y_pred_scores = y_pred_scores[:, 1]\n",
    "        auroc = roc_auc_score(y, y_pred_scores)\n",
    "        aupr = average_precision_score(y, y_pred_scores)\n",
    "        performance_metrics = performance_metrics + (auroc, aupr)\n",
    "    return performance_metrics\n",
    "\n",
    "def print_metrics_summary(accuracy, recall, precision, f1, auroc=None, aupr=None):\n",
    "    print()\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Accuracy:\", value=accuracy))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Recall:\", value=recall))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Precision:\", value=precision))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"F1:\", value=f1))\n",
    "    if auroc is not None:\n",
    "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUROC:\", value=auroc))\n",
    "    if aupr is not None:\n",
    "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUPR:\", value=aupr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo paths e carregabdo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/pedro/projetoDL/dataset/processado/\"\n",
    "save_path = \"/home/pedro/projetoDL/log/prunning/new-prunning/\"\n",
    "\n",
    "# Verifica se o diretório existe, se não, cria o diretório\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Carregar os dados\n",
    "Y = np.load(path + 'Y_train_NewApproach_Injected_v2.npz')\n",
    "Y= Y.f.arr_0\n",
    "\n",
    "X = np.load(path + 'X_train_NewApproach_Injected_v2.npz')\n",
    "X = X.f.arr_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/pedro/projetoDL/dataset/processado/\"\n",
    "\n",
    "# Y = np.load(path + 'Y_test_Driving_NewApproach_Injected_v2.npz')\n",
    "# Y= Y.f.arr_0\n",
    "\n",
    "# X = np.load(path + 'X_test_Driving_NewApproach_Injected_v2.npz')\n",
    "# X = X.f.arr_0\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Definir 2/3 dos dados\n",
    "# X_subset, _, Y_subset, _ = train_test_split(\n",
    "#     X, Y, train_size=1/3, stratify=Y, random_state=42  # Stratify para manter a proporção de classes\n",
    "# )\n",
    "\n",
    "# # Agora X_subset e Y_subset têm 2/3 dos dados originais\n",
    "# X_test = X_subset\n",
    "# Y_test = Y_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecionar os dados do fold 3 (Melhor resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Configurar o KFold e selecionar o fold 3\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_no = 0\n",
    "\n",
    "for train_index, val_index in skf.split(X, Y):\n",
    "    fold_no += 1\n",
    "    if fold_no == 3:  # Selecionar o fold 3\n",
    "        x_train, x_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = Y[train_index], Y[val_index]\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# Verificar os dados\n",
    "print(f\"Tamanho de x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Tamanho de x_val: {x_val.shape}, y_val: {y_val.shape}\")\n",
    "print(\"Distribuição em y_train:\", np.unique(y_train, return_counts=True))\n",
    "print(\"Distribuição em y_val:\", np.unique(y_val, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinar o modelo sem prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "from PIL import Image\n",
    "\n",
    "# Configurar o mixed precision\n",
    "# Equivalent to the two lines above\n",
    "keras.mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# Definir o modelo Sequential\n",
    "model = keras.Sequential([\n",
    "    # 1ª camada convolucional\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu',\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01), padding='same', input_shape=(44, 116, 1)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # 2ª camada convolucional\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=5, strides=(1, 1), activation='relu',\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01), padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Camada de Flatten para transformar os dados em vetor\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    # Camadas densas (fully connected)\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(1, activation='sigmoid', dtype='float32')  # Saída para classificação binária\n",
    "])\n",
    "\n",
    "# Mostrar resumo do modelo\n",
    "print(model.summary())\n",
    "\n",
    "# Gerar visualização do modelo\n",
    "# keras.utils.plot_model(model, to_file=\"conv2d.png\", show_shapes=True, show_layer_names=True)\n",
    "# display(Image.open('conv2d.png'))\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    factor=0.5,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Treinando o modelo\n",
    "batch_size = 256\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(x_val, y_val),\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[learning_rate_reduction, early_stopping],\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = save_path + \"full/\"\n",
    "\n",
    "# Verifica se o diretório existe, se não, cria o diretório\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "\n",
    "keras_file = model_save_path + 'original_model.h5'\n",
    "keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "print('Saved full baseline model to:', keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliar o desempenho e salvar o modelo pra uso posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# import scikitplot as skplt\n",
    "\n",
    "file_name = f'log_original_model.txt'\n",
    "file1 = open(os.path.join(model_save_path, file_name), \"a\")\n",
    "\n",
    "start = time.time()\n",
    "y_pred_scores = model.predict(x_val)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Runtime of the program is {end - start}\", file=file1)\n",
    "total_time = end - start\n",
    "timesample = total_time/y_pred_scores.shape[0]\n",
    "print(f\"us/sample is {timesample*1000000}\", file=file1)\n",
    "\n",
    "y_pred_class = (y_pred_scores > 0.5).astype(\"int64\")\n",
    "\n",
    "y_pred_scores2 = y_pred_scores\n",
    "y_pred_scores_0 = 1 - y_pred_scores\n",
    "y_pred_scores = np.concatenate([y_pred_scores_0, y_pred_scores], axis=1)\n",
    "\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, y_pred_class, y_pred_scores)\n",
    "\n",
    "mseTeste = mean_squared_error(y_val, y_pred_class)\n",
    "\n",
    "print(f'Results for original_model: Recall of {recall}; accuracy of {accuracy}; precision of {precision}; f1 of {f1}; auroc of {auroc}; aupr of {aupr}', file=file1)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(model_save_path + \"loss_and_val_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['accuracy'], label='Train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(model_save_path + \"loss_and_val_accuracy.png\")\n",
    "plt.show()\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_val, y_pred_scores2)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "plt.plot(fpr_keras, tpr_keras, marker='.', label='MLP (auc = %0.3f)' % auc_keras)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(model_save_path + \"plot_roc.png\")\n",
    "plt.show()\n",
    "#########################################################################################################\n",
    "#Plotando o histórico do treino\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o modelo se nao carregar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732584558.815743 1169999 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1732584558.815890 1169999 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1732584558.815943 1169999 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1732584558.914806 1169999 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1732584558.914934 1169999 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1732584558.914993 1169999 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-25 22:29:18.915047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5460 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "\n",
    "# Carregar o modelo\n",
    "model_path = '/home/pedro/projetoDL/log/prunning/new-prunning/full/original_model.h5'\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune pre-trained model with prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "\n",
    "# Pruning: Reduzir a magnitude de pesos irrelevantes no modelo\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Definir parâmetros de pruning\n",
    "batch_size = 256\n",
    "epochs = 6\n",
    "validation_split = 0.1  # Usar 10% do conjunto de treinamento para validação\n",
    "\n",
    "# Calcular o número de passos para finalizar o pruning após 6 épocas\n",
    "num_images = x_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Definir parâmetros de pruning\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.50,  # Começar com 50% de pesos zerados\n",
    "        final_sparsity=0.90,    # Finalizar com 80% de pesos zerados\n",
    "        begin_step=0,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "\n",
    "# Envolver o modelo com poda\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# Compilar o modelo\n",
    "model_for_pruning.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Treinar o modelo com callbacks de poda\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', patience=3, verbose=1, factor=0.5, min_lr=0.00001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "]\n",
    "\n",
    "# Treinar o modelo com pruning\n",
    "history_pruning = model_for_pruning.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prunned_save_path = save_path + \"prunned/\"\n",
    "\n",
    "# # Verifica se o diretório existe, se não, cria o diretório\n",
    "# if not os.path.exists(model_prunned_save_path):\n",
    "#     os.makedirs(model_prunned_save_path)\n",
    "\n",
    "\n",
    "# keras_file = model_prunned_save_path + 'prunned.h5'\n",
    "# keras.models.save_model(model_for_pruning, keras_file, include_optimizer=False)\n",
    "# print('Saved prunned baseline model to:', keras_file)\n",
    "\n",
    "\n",
    "# Remover wrappers de poda para exportação\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "# Salvar como H5 para GPU\n",
    "model_for_export.save(model_prunned_save_path + \"model_pruned_gpu.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Ja treinados (carregar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/pedro/projetoDL/log/prunning/new-prunning/prunned/model_pruned_gpu.h5'\n",
    "model_for_pruning = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo com pruning (model_for_pruning):\n",
      "Parâmetros treináveis: 1359041\n",
      "Parâmetros não treináveis: 192\n"
     ]
    }
   ],
   "source": [
    "# Calcular o número de parâmetros para o modelo original (model)\n",
    "# trainable_params_model = sum(np.prod(weight.shape) for weight in model.trainable_weights)\n",
    "# non_trainable_params_model = sum(np.prod(weight.shape) for weight in model.non_trainable_weights)\n",
    "\n",
    "# print(f\"Modelo original (model):\")\n",
    "# print(f\"Parâmetros treináveis: {trainable_params_model}\")\n",
    "# print(f\"Parâmetros não treináveis: {non_trainable_params_model}\")\n",
    "\n",
    "# Calcular o número de parâmetros para o modelo com pruning (model_for_pruning)\n",
    "trainable_params_pruning = sum(np.prod(weight.shape) for weight in model_for_pruning.trainable_weights)\n",
    "non_trainable_params_pruning = sum(np.prod(weight.shape) for weight in model_for_pruning.non_trainable_weights)\n",
    "\n",
    "print(f\"\\nModelo com pruning (model_for_pruning):\")\n",
    "print(f\"Parâmetros treináveis: {trainable_params_pruning}\")\n",
    "print(f\"Parâmetros não treináveis: {non_trainable_params_pruning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsidade do modelo: 89.98%\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(np.prod(weight.shape) for weight in model_for_pruning.trainable_weights)\n",
    "nonzero_params = sum(np.count_nonzero(weight.numpy()) for weight in model_for_pruning.trainable_weights)\n",
    "\n",
    "sparsity = 1 - (nonzero_params / total_params)\n",
    "print(f\"Sparsidade do modelo: {sparsity * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros antes da poda: 1359041\n",
      "Parâmetros após a poda: 1359041\n",
      "Parâmetros removidos: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Função para calcular o número de parâmetros treináveis\n",
    "def get_model_params(model):\n",
    "    return np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
    "\n",
    "# Calcular o número de parâmetros antes e depois da poda\n",
    "params_before_pruning = get_model_params(model)\n",
    "params_after_pruning = get_model_params(model_for_export)\n",
    "\n",
    "# Mostrar os resultados\n",
    "print(f\"Parâmetros antes da poda: {params_before_pruning}\")\n",
    "print(f\"Parâmetros após a poda: {params_after_pruning}\")\n",
    "print(f\"Parâmetros removidos: {params_before_pruning - params_after_pruning} \"\n",
    "      f\"({(params_before_pruning - params_after_pruning) / params_before_pruning * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate prunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prunned_save_path = save_path + \"prunned/\"\n",
    "\n",
    "file_name = f'log_modelo_fold3.txt'\n",
    "file1 = open(os.path.join(model_prunned_save_path, file_name), \"a\")\n",
    "\n",
    "start = time.time()\n",
    "y_pred_scores = model_for_pruning.predict(x_val)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Runtime of the program is {end - start}\", file=file1)\n",
    "total_time = end - start\n",
    "timesample = total_time/y_pred_scores.shape[0]\n",
    "\n",
    "y_pred_class = (y_pred_scores > 0.5).astype(\"int64\")\n",
    "\n",
    "y_pred_scores2 = y_pred_scores\n",
    "y_pred_scores_0 = 1 - y_pred_scores\n",
    "y_pred_scores = np.concatenate([y_pred_scores_0, y_pred_scores], axis=1)\n",
    "\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, y_pred_class, y_pred_scores)\n",
    "\n",
    "\n",
    "mseTeste = mean_squared_error(y_val, y_pred_class)\n",
    "\n",
    "print(f'Results for Prunned Model [89% sparsity]:\\n Recall of {recall}; accuracy of {accuracy}; precision of {precision}; f1 of {f1}; auroc of {auroc}; aupr of {aupr}', file=file1)\n",
    "print(f\"us/sample is {timesample*1000000}\", file=file1)\n",
    "\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(8,8)})\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.savefig(model_prunned_save_path + \"loss_and_val_loss_prunned.png\")\n",
    "# plt.show()\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(8,8)})\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.plot(history.history['accuracy'], label='Train accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.savefig(model_prunned_save_path + \"loss_and_val_accuracy_prunned.png\")\n",
    "# plt.show()\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(8,8)})\n",
    "# subplot = skplt.metrics.plot_confusion_matrix(y_val, y_pred_class, normalize=True)\n",
    "# subplot.set_ylim(-0.5, 1.5)\n",
    "# plt.savefig(save_path + \"conf_matrix.png\")\n",
    "# plt.show()\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_val, y_pred_scores2)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "plt.plot(fpr_keras, tpr_keras, marker='.', label='MLP (auc = %0.3f)' % auc_keras)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(model_prunned_save_path + \"plot_roc_prunned.png\")\n",
    "plt.show()\n",
    "#########################################################################################################\n",
    "#Plotando o histórico do treino\n",
    "# pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "# plt.grid(True)\n",
    "# plt.gca().set_ylim(0, 1)\n",
    "# plt.show()\n",
    "\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 3x smaller models from pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "model_3xprunned_save_path = save_path + \"prunned/\"\n",
    "\n",
    "# Verifica se o diretório existe, se não, cria o diretório\n",
    "if not os.path.exists(model_prunned_save_path):\n",
    "    os.makedirs(model_prunned_save_path)\n",
    "\n",
    "pruned_keras_file = model_3xprunned_save_path + '3xPrunned.h5'\n",
    "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prunned_save_path = save_path + \"prunned/\"\n",
    "\n",
    "file_name = f'log_modelo_fold3.txt'\n",
    "file1 = open(os.path.join(model_prunned_save_path, file_name), \"a\")\n",
    "\n",
    "start = time.time()\n",
    "y_pred_scores = model_for_export.predict(x_val)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Runtime of the program is {end - start}\", file=file1)\n",
    "total_time = end - start\n",
    "timesample = total_time/y_pred_scores.shape[0]\n",
    "\n",
    "y_pred_class = (y_pred_scores > 0.5).astype(\"int64\")\n",
    "\n",
    "y_pred_scores2 = y_pred_scores\n",
    "y_pred_scores_0 = 1 - y_pred_scores\n",
    "y_pred_scores = np.concatenate([y_pred_scores_0, y_pred_scores], axis=1)\n",
    "\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, y_pred_class, y_pred_scores)\n",
    "\n",
    "\n",
    "mseTeste = mean_squared_error(y_val, y_pred_class)\n",
    "\n",
    "print(f'\\nResults for Prunned Model [89% sparsity - 3x prunned]:\\n Recall of {recall}; accuracy of {accuracy}; precision of {precision}; f1 of {f1}; auroc of {auroc}; aupr of {aupr}', file=file1)\n",
    "print(f\"us/sample is {timesample*1000000}\", file=file1)\n",
    "\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_val, y_pred_scores2)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "plt.plot(fpr_keras, tpr_keras, marker='.', label='MLP (auc = %0.3f)' % auc_keras)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(model_prunned_save_path + \"plot_roc_prunned.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3x prunned tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "pruned_tflite_file = model_3xprunned_save_path + 'TFLite_3xPrunned.tflite'\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular o número de parâmetros treináveis\n",
    "def get_model_params(model):\n",
    "    return np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
    "\n",
    "# Calcular o número de parâmetros antes e depois da poda\n",
    "params_before_pruning = get_model_params(model)\n",
    "params_after_pruning = get_model_params(model_for_export)\n",
    "\n",
    "# Mostrar os resultados\n",
    "print(f\"Parâmetros antes da poda: {params_before_pruning}\")\n",
    "print(f\"Parâmetros após a poda: {params_after_pruning}\")\n",
    "print(f\"Parâmetros removidos: {params_before_pruning - params_after_pruning} \"\n",
    "      f\"({(params_before_pruning - params_after_pruning) / params_before_pruning * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate prunned tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Salvar uma pequena parte dos dados\n",
    "small_x_val = x_val[:100]  # Pegando as 100 primeiras amostras\n",
    "small_y_val = y_val[:100]  # Pegando os rótulos correspondentes\n",
    "\n",
    "np.save(\"small_x_val.npy\", small_x_val)\n",
    "np.save(\"small_y_val.npy\", small_y_val)\n",
    "\n",
    "# Modelo TFLite já está em `pruned_tflite_file`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carregar o modelo TFLite\n",
    "# interpreter = tf.lite.Interpreter(model_path=pruned_tflite_file)\n",
    "#interpreter.allocate_tensors()\n",
    "# Criar o interpretador com múltiplos threads\n",
    "interpreter = tf.lite.Interpreter(\n",
    "    model_path=pruned_tflite_file,\n",
    "    num_threads=12  # Defina o número desejado de threads\n",
    ")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "\n",
    "# Pegar detalhes do tensor de entrada e saída\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Certifique-se de que x_val está no formato esperado\n",
    "x_val_tflite = np.expand_dims(x_val, axis=-1)  # Adicionar dimensão do canal\n",
    "x_val_tflite = x_val_tflite.astype(np.float32)  # Garantir que o tipo é float32\n",
    "\n",
    "# Fazer previsões com o modelo TFLite\n",
    "y_pred_scores_tflite = []\n",
    "start = time.time()\n",
    "\n",
    "for sample in x_val_tflite:\n",
    "    sample = np.expand_dims(sample, axis=0)  # Adicionar dimensão batch\n",
    "    interpreter.set_tensor(input_details[0]['index'], sample)\n",
    "    interpreter.invoke()\n",
    "    prediction = interpreter.get_tensor(output_details[0]['index'])\n",
    "    y_pred_scores_tflite.append(prediction)\n",
    "\n",
    "end = time.time()\n",
    "y_pred_scores_tflite = np.array(y_pred_scores_tflite).squeeze()\n",
    "\n",
    "# Calcular o tempo médio por amostra\n",
    "total_time_tflite = end - start\n",
    "timesample_tflite = total_time_tflite / y_pred_scores_tflite.shape[0]\n",
    "\n",
    "# Classe prevista\n",
    "y_pred_class_tflite = (y_pred_scores_tflite > 0.5).astype(\"int64\")\n",
    "\n",
    "# Ajustar as previsões para um formato bidimensional\n",
    "y_pred_scores_tflite = np.expand_dims(y_pred_scores_tflite, axis=1)  # Torna bidimensional: [N] -> [N, 1]\n",
    "y_pred_scores_tflite_0 = 1 - y_pred_scores_tflite  # Probabilidades da classe 0\n",
    "\n",
    "# Concatenar probabilidades para cada classe\n",
    "y_pred_scores_tflite_concat = np.concatenate([y_pred_scores_tflite_0, y_pred_scores_tflite], axis=1)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy_tflite, recall_tflite, precision_tflite, f1_tflite, auroc_tflite, aupr_tflite = compute_performance_metrics(\n",
    "    y_val, y_pred_class_tflite, y_pred_scores_tflite_concat\n",
    ")\n",
    "mseTeste_tflite = mean_squared_error(y_val, y_pred_class_tflite)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"Runtime of the TFLite model is {total_time_tflite}\")\n",
    "print(f\"Time per sample is {timesample_tflite * 1e6:.2f} us/sample\")\n",
    "print(f\"\\nResults for Pruned TFLite Model:\\n Recall: {recall_tflite}; Accuracy: {accuracy_tflite}; Precision: {precision_tflite}; F1: {f1_tflite}; AUROC: {auroc_tflite}; AUPR: {aupr_tflite}\")\n",
    "\n",
    "# Curva ROC\n",
    "fpr_tflite, tpr_tflite, thresholds_tflite = roc_curve(y_val, y_pred_scores_tflite)\n",
    "auc_tflite = auc(fpr_tflite, tpr_tflite)\n",
    "\n",
    "sns.set(rc={'figure.figsize': (8, 8)})\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='black')\n",
    "plt.plot(fpr_tflite, tpr_tflite, marker='.', label='TFLite (auc = %0.3f)' % auc_tflite)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC - TFLite Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(model_prunned_save_path + \"plot_roc_tflite_prunned.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  zipped_file = model_3xprunned_save_path + '3xPrunned.zip'\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "# print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "# print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 10x smaller model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10xprunned_save_path = save_path + \"10xPrunned/\"\n",
    "\n",
    "# Verifica se o diretório existe, se não, cria o diretório\n",
    "if not os.path.exists(model_10xprunned_save_path):\n",
    "    os.makedirs(model_10xprunned_save_path)\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "quantized_and_pruned_tflite_file = model_10xprunned_save_path + \"10xPrunned_TFLite.tflite\"\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "  f.write(quantized_and_pruned_tflite_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
    "\n",
    "# print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate 10x smaller model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def evaluate_model(interpreter, x_val, y_val):\n",
    "    # Obter detalhes de entrada e saída do modelo TFLite\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_index = input_details[0][\"index\"]\n",
    "    output_index = output_details[0][\"index\"]\n",
    "\n",
    "    # Verificar tipo de dado esperado pelo modelo\n",
    "    input_dtype = input_details[0][\"dtype\"]\n",
    "\n",
    "    # Garantir que x_val tenha 4 dimensões (batch_size, height, width, channels)\n",
    "    if len(x_val.shape) == 3:\n",
    "        x_val = np.expand_dims(x_val, axis=-1)  # Adiciona dimensão do canal\n",
    "\n",
    "    # Lista para armazenar as previsões\n",
    "    prediction_classes = []\n",
    "\n",
    "    print(\"Iniciando avaliação...\")\n",
    "\n",
    "    # Medir o tempo total de inferência (apenas dentro de interpreter.invoke())\n",
    "    total_inference_time = 0.0\n",
    "\n",
    "    for i, val_image in enumerate(x_val):\n",
    "        if i % 10000 == 0 and i > 0:\n",
    "            print(f\"Avaliado em {i} amostras até agora.\")\n",
    "\n",
    "        # Pré-processamento: adicionar dimensão do batch e ajustar tipo de dado\n",
    "        val_image = np.expand_dims(val_image, axis=0).astype(input_dtype)\n",
    "\n",
    "        # Definir a entrada no modelo\n",
    "        interpreter.set_tensor(input_index, val_image)\n",
    "\n",
    "        # Medir o tempo de inferência\n",
    "        start_inference = time.time()\n",
    "        interpreter.invoke()\n",
    "        end_inference = time.time()\n",
    "\n",
    "        # Somar o tempo de inferência\n",
    "        total_inference_time += (end_inference - start_inference)\n",
    "\n",
    "        # Pós-processamento: obter a classe prevista\n",
    "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "        predicted_class = np.argmax(output[0])\n",
    "        prediction_classes.append(predicted_class)\n",
    "\n",
    "    print(\"Avaliação finalizada.\")\n",
    "\n",
    "    # Comparar as previsões com as classes reais para calcular a acurácia\n",
    "    prediction_classes = np.array(prediction_classes)\n",
    "    # Se y_val já for 1D, não use np.argmax\n",
    "    if len(y_val.shape) == 1:  # Verifica se y_val é 1D\n",
    "        y_val_classes = y_val\n",
    "    else:  # Se for one-hot encoded\n",
    "        y_val_classes = np.argmax(y_val, axis=1)\n",
    "    accuracy = (prediction_classes == y_val_classes).mean()\n",
    "\n",
    "    # Calcular o tempo médio de inferência por amostra\n",
    "    num_samples = x_val.shape[0]\n",
    "    inference_time_per_sample_us = (total_inference_time / num_samples) * 1e6  # Microssegundos\n",
    "\n",
    "    print(f\"Acurácia: {accuracy:.4f}\")\n",
    "    print(f\"Tempo médio de inferência por amostra (somente invoke): {inference_time_per_sample_us:.2f} µs\")\n",
    "\n",
    "    return accuracy, inference_time_per_sample_us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o modelo TFLite\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "\n",
    "test_accuracy, inference_time = evaluate_model(interpreter, x_val, y_val)\n",
    "\n",
    "\n",
    "# Exibir resultados\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
    "print('Inference time per sample for pruned model: {:.2f} microseconds'.format(inference_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = f'log_modelo_fold3.txt'\n",
    "# file1 = open(os.path.join(model_10xprunned_save_path, file_name), \"a\")\n",
    "\n",
    "# start = time.time()\n",
    "# y_pred_scores = model.predict(x_val)\n",
    "# end = time.time()\n",
    "\n",
    "# print(f\"Runtime of the program is {end - start}\", file=file1)\n",
    "# total_time = end - start\n",
    "# timesample = total_time/y_pred_scores.shape[0]\n",
    "# print(f\"us/sample is {timesample*1000000}\", file=file1)\n",
    "\n",
    "# y_pred_class = (y_pred_scores > 0.5).astype(\"int64\")\n",
    "\n",
    "# y_pred_scores2 = y_pred_scores\n",
    "# y_pred_scores_0 = 1 - y_pred_scores\n",
    "# y_pred_scores = np.concatenate([y_pred_scores_0, y_pred_scores], axis=1)\n",
    "\n",
    "# accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, y_pred_class, y_pred_scores)\n",
    "\n",
    "\n",
    "# mseTeste = mean_squared_error(y_val, y_pred_class)\n",
    "\n",
    "# print(f'Results for Prunned Model [10x]:\\n Recall of {recall}; accuracy of {accuracy}; precision of {precision}; f1 of {f1}; auroc of {auroc}; aupr of {aupr}', file=file1)\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(8,8)})\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.savefig(model_10xprunned_save_path + \"loss_and_val_loss_prunned.png\")\n",
    "# plt.show()\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(8,8)})\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.plot(history.history['accuracy'], label='Train accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.savefig(model_10xprunned_save_path + \"loss_and_val_accuracy_prunned.png\")\n",
    "# plt.show()\n",
    "\n",
    "# # sns.set(rc={'figure.figsize':(8,8)})\n",
    "# # subplot = skplt.metrics.plot_confusion_matrix(y_val, y_pred_class, normalize=True)\n",
    "# # subplot.set_ylim(-0.5, 1.5)\n",
    "# # plt.savefig(save_path + \"conf_matrix.png\")\n",
    "# # plt.show()\n",
    "\n",
    "# fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_val, y_pred_scores2)\n",
    "# auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(8,8)})\n",
    "# plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "# plt.plot(fpr_keras, tpr_keras, marker='.', label='MLP (auc = %0.3f)' % auc_keras)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig(model_10xprunned_save_path + \"plot_roc_prunned.png\")\n",
    "# plt.show()\n",
    "# #########################################################################################################\n",
    "# #Plotando o histórico do treino\n",
    "# pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "# plt.grid(True)\n",
    "# plt.gca().set_ylim(0, 1)\n",
    "# plt.show()\n",
    "\n",
    "# file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a more optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 21:16:55.903921: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "W0000 00:00:1732580215.953236 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580215.982669 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580215.984477 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580215.989378 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580215.991353 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.005295 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.009717 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.012589 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.015411 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.029200 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.033694 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.043038 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.092371 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.098688 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.142672 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.148367 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.154159 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.159418 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.167697 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.175151 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.182638 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.189062 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.195902 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.203492 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.213146 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.218620 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.226751 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.236560 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.252343 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580216.261202 1140595 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732580216.941807 1140603 service.cc:146] XLA service 0x7065308989c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732580216.941822 1140603 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4060, Compute Capability 8.9\n",
      "2024-11-25 21:16:56.959775: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-25 21:16:57.007256: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1732580217.029208 1140603 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "W0000 00:00:1732580217.200815 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.206894 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.213116 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.220218 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.226367 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.233309 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.241792 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.253838 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.264380 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.274772 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.287102 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.298739 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.309260 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.327376 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.340322 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.348669 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.353740 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.371497 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.386955 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.403648 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.503653 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.508376 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.513534 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.520972 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.525825 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.533829 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.540813 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.548286 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.555581 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.562813 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.571773 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.584787 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.594280 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.602962 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.612674 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.624030 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.639942 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.659458 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.669615 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.677781 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.691628 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.939530 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.943251 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.945781 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.949134 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.952295 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.957222 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.960747 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.965976 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.970166 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.975073 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.981081 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.987458 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580217.993078 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580218.002023 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580218.007434 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580218.011002 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580218.027380 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580218.126734 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1809/1810 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9972"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732580293.934879 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.935836 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.936776 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.937778 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.938782 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.940542 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.942300 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.943973 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.945208 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.950397 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.952108 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.953951 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.956067 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.957800 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.967690 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.969614 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.971548 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.973471 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.976353 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.979091 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.982296 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.984502 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.987331 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.989865 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.993559 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.995385 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580293.998172 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.001518 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.006429 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.010273 1140602 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.033750 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.036256 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.038647 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.040958 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.043817 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.046106 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.051457 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.054370 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.057856 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.062645 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.065477 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.069276 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.072849 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.076849 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.081055 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.084721 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.089926 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.095413 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.101009 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.127595 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.129627 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.132441 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.134340 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1810/1810 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9972"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732580294.137246 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.139852 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.142390 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.145253 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.148661 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.152738 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.155387 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.157507 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.160747 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.164794 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.168443 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.171984 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.177658 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.184441 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.187088 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.190874 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.194872 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.232728 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.234814 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.236059 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.237340 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.238749 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.240673 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.242230 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.243917 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.245721 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.248340 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.250468 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.252763 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.254903 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.258745 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.261064 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.262604 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.268898 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.272126 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580294.280187 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1810/1810 [==============================] - 84s 44ms/step - loss: 0.0177 - accuracy: 0.9972 - val_loss: 0.0189 - val_accuracy: 0.9965 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "   3/1810 [..............................] - ETA: 1:11 - loss: 0.0283 - accuracy: 0.9935"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732580297.341171 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.341899 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.342483 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.343074 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.343659 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.344388 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.345129 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.345745 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.346364 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.347936 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.348603 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.349649 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.350513 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.351350 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.354952 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.355756 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.356483 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.357266 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.358191 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.359319 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.360076 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.360843 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.361546 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.362635 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.364297 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.365198 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.367136 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1732580297.368376 1140603 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1810/1810 [==============================] - 78s 43ms/step - loss: 0.0183 - accuracy: 0.9976 - val_loss: 0.0166 - val_accuracy: 0.9979 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1810/1810 [==============================] - 79s 43ms/step - loss: 0.0194 - accuracy: 0.9973 - val_loss: 0.0143 - val_accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1810/1810 [==============================] - 77s 43ms/step - loss: 0.0229 - accuracy: 0.9966 - val_loss: 0.0201 - val_accuracy: 0.9981 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1810/1810 [==============================] - 78s 43ms/step - loss: 0.0329 - accuracy: 0.9945 - val_loss: 0.0214 - val_accuracy: 0.9971 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1809/1810 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9872\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1810/1810 [==============================] - 78s 43ms/step - loss: 0.0614 - accuracy: 0.9872 - val_loss: 0.0241 - val_accuracy: 0.9980 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1810/1810 [==============================] - 78s 43ms/step - loss: 0.0751 - accuracy: 0.9823 - val_loss: 0.0268 - val_accuracy: 0.9977 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "1810/1810 [==============================] - 68s 38ms/step - loss: 0.1133 - accuracy: 0.9680 - val_loss: 0.0315 - val_accuracy: 0.9975 - lr: 5.0000e-04\n",
      "Modelo podado salvo em: ./prunned_model/Pruned_Model.h5\n",
      "Modelo TFLite podado salvo em: ./prunned_model/Pruned_Model.tflite\n",
      "Tamanho do arquivo './prunned_model/Pruned_Model.h5': 5344.56 KB\n",
      "Tamanho do arquivo './prunned_model/Pruned_Model.tflite': 1336.56 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732580833.857282 1138323 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1732580833.857293 1138323 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-11-25 21:27:13.857488: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpajw0v_ky\n",
      "2024-11-25 21:27:13.858372: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-25 21:27:13.858382: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpajw0v_ky\n",
      "2024-11-25 21:27:13.862814: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-11-25 21:27:13.863594: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-25 21:27:13.885939: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpajw0v_ky\n",
      "2024-11-25 21:27:13.892705: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 35218 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1368640"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# **1. Configuração inicial e parâmetros gerais**\n",
    "batch_size = 256  # Tamanho do lote para treinamento\n",
    "epochs = 10       # Aumentar para garantir que a poda seja concluída\n",
    "validation_split = 0.1  # Usar 10% dos dados para validação\n",
    "\n",
    "# Calcular o número de passos para concluir a poda\n",
    "num_images = x_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# **2. Configuração da poda (pruning)**\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.7,  # Começa com 70% de sparsidade\n",
    "        final_sparsity=0.95,   # Termina com 95% de sparsidade\n",
    "        begin_step=0,          # Poda ativa desde o início\n",
    "        end_step=end_step      # Finaliza após todas as épocas\n",
    "    )\n",
    "}\n",
    "\n",
    "# Aplicar poda ao modelo\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# **3. Compilar o modelo podado**\n",
    "model_for_pruning.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# **4. Configurar callbacks**\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),  # Atualiza os pesos podados a cada passo\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', patience=3, verbose=1, factor=0.5, min_lr=1e-5\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),  # Interrompe cedo se não houver melhora\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs/pruning', update_freq='batch')  # Logs para depuração\n",
    "]\n",
    "\n",
    "# **5. Treinamento do modelo podado**\n",
    "history = model_for_pruning.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# **6. Remover wrappers de poda após treinamento**\n",
    "# Isso prepara o modelo para exportação e uso em produção\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "# **7. Exportar o modelo podado no formato Keras**\n",
    "pruned_model_path = \"./prunned_model/\"\n",
    "if not os.path.exists(pruned_model_path):\n",
    "    os.makedirs(pruned_model_path)\n",
    "\n",
    "pruned_keras_file = pruned_model_path + \"Pruned_Model.h5\"\n",
    "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print(\"Modelo podado salvo em:\", pruned_keras_file)\n",
    "\n",
    "# **8. Converter o modelo podado para TFLite**\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "\n",
    "# Ativar sparsidade otimizada no TFLite\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Converter para o formato TFLite\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "# Salvar o modelo TFLite otimizado\n",
    "pruned_tflite_file = pruned_model_path + \"Pruned_Model.tflite\"\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "    f.write(pruned_tflite_model)\n",
    "\n",
    "print(\"Modelo TFLite podado salvo em:\", pruned_tflite_file)\n",
    "\n",
    "# **9. Verificar o tamanho do modelo**\n",
    "def get_model_size(file_path):\n",
    "    import os\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"Tamanho do arquivo '{file_path}': {size / 1024:.2f} KB\")\n",
    "    return size\n",
    "\n",
    "get_model_size(pruned_keras_file)\n",
    "get_model_size(pruned_tflite_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mais uma tentativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1810/1810 [==============================] - 80s 44ms/step - loss: 0.0192 - accuracy: 0.9973 - val_loss: 0.0129 - val_accuracy: 0.9985 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1810/1810 [==============================] - 78s 43ms/step - loss: 0.0161 - accuracy: 0.9979 - val_loss: 0.0174 - val_accuracy: 0.9975 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1810/1810 [==============================] - 79s 43ms/step - loss: 0.0183 - accuracy: 0.9976 - val_loss: 0.0204 - val_accuracy: 0.9975 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1810/1810 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9973\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1810/1810 [==============================] - 78s 43ms/step - loss: 0.0208 - accuracy: 0.9973 - val_loss: 0.0230 - val_accuracy: 0.9977 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1810/1810 [==============================] - 79s 43ms/step - loss: 0.0219 - accuracy: 0.9969 - val_loss: 0.0204 - val_accuracy: 0.9982 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "1810/1810 [==============================] - 79s 43ms/step - loss: 0.0343 - accuracy: 0.9939 - val_loss: 0.0141 - val_accuracy: 0.9985 - lr: 5.0000e-04\n",
      "Modelo podado salvo em: ./prunned_model_new/Pruned_Model_new.h5\n",
      "Modelo TFLite podado salvo em: ./prunned_model_new/Pruned_Model_new.tflite\n",
      "Tamanho do arquivo './prunned_model_new/Pruned_Model_new.h5': 5344.56 KB\n",
      "Tamanho do arquivo './prunned_model_new/Pruned_Model_new.tflite': 5313.08 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732582649.054961 1138323 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1732582649.054971 1138323 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-11-25 21:57:29.055069: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp9ag135hs\n",
      "2024-11-25 21:57:29.055924: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-25 21:57:29.055935: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp9ag135hs\n",
      "2024-11-25 21:57:29.060809: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-25 21:57:29.080900: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp9ag135hs\n",
      "2024-11-25 21:57:29.087447: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 32378 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5440596"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# **1. Configuração inicial**\n",
    "batch_size = 256  # Tamanho do lote para treinamento\n",
    "epochs = 10       # Número de épocas para garantir o efeito da poda\n",
    "validation_split = 0.1  # Usar 10% dos dados para validação\n",
    "\n",
    "# Calcular o número de passos para concluir a poda\n",
    "num_images = x_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# **2. Configuração da poda**\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.7,  # Começa com 70% de sparsidade\n",
    "        final_sparsity=0.95,   # Termina com 95% de sparsidade\n",
    "        begin_step=0,          # Poda ativa desde o início\n",
    "        end_step=end_step      # Finaliza após todas as épocas\n",
    "    )\n",
    "}\n",
    "\n",
    "# Aplicar poda estrutural ao modelo\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# **3. Compilar o modelo**\n",
    "model_for_pruning.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# **4. Configurar callbacks**\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),  # Atualiza os pesos podados a cada passo\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', patience=3, verbose=1, factor=0.5, min_lr=1e-5\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),  # Interrompe cedo se não houver melhora\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs/pruning', update_freq='batch')  # Logs para depuração\n",
    "]\n",
    "\n",
    "# **5. Treinamento do modelo podado**\n",
    "history = model_for_pruning.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# **6. Remover wrappers de poda após o treinamento**\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "# **7. Reestruturar o modelo para remover pesos zerados**\n",
    "def restructured_model(model):\n",
    "    \"\"\"Reestrutura o modelo, removendo pesos zerados e otimizando camadas.\"\"\"\n",
    "    restructured_layers = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):\n",
    "            # Apenas reestrutura camadas densas ou convolucionais\n",
    "            weights, biases = layer.get_weights()\n",
    "            weights = weights[weights != 0]  # Remove pesos zerados\n",
    "            if weights.size > 0:  # Apenas adiciona camadas com pesos ativos\n",
    "                restructured_layers.append(layer)\n",
    "        else:\n",
    "            # Adiciona outras camadas diretamente\n",
    "            restructured_layers.append(layer)\n",
    "    return tf.keras.Sequential(restructured_layers)\n",
    "\n",
    "compact_model = restructured_model(model_for_export)\n",
    "\n",
    "# **8. Exportar o modelo podado no formato Keras**\n",
    "pruned_model_path = \"./prunned_model_new/\"\n",
    "if not os.path.exists(pruned_model_path):\n",
    "    os.makedirs(pruned_model_path)\n",
    "\n",
    "pruned_keras_file = pruned_model_path + \"Pruned_Model_new.h5\"\n",
    "compact_model.save(pruned_keras_file, include_optimizer=False)\n",
    "print(\"Modelo podado salvo em:\", pruned_keras_file)\n",
    "\n",
    "# **9. Converter para TFLite sem quantização**\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(compact_model)\n",
    "\n",
    "# Conversão simples para TFLite\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "# Salvar o modelo TFLite otimizado\n",
    "pruned_tflite_file = pruned_model_path + \"Pruned_Model_new.tflite\"\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "    f.write(pruned_tflite_model)\n",
    "\n",
    "print(\"Modelo TFLite podado salvo em:\", pruned_tflite_file)\n",
    "\n",
    "# **10. Verificar o tamanho do modelo**\n",
    "def get_model_size(file_path):\n",
    "    \"\"\"Calcula o tamanho do modelo salvo.\"\"\"\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"Tamanho do arquivo '{file_path}': {size / 1024:.2f} KB\")\n",
    "    return size\n",
    "\n",
    "get_model_size(pruned_keras_file)\n",
    "get_model_size(pruned_tflite_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mais uma tentativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando poda ao modelo...\n",
      "Compilando...\n",
      "Treinando o modelo...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PruneLowMagnitude' object has no attribute 'pruning_step'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 77\u001b[0m\n\u001b[1;32m     68\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     69\u001b[0m     tfmot\u001b[38;5;241m.\u001b[39msparsity\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mUpdatePruningStep(),\n\u001b[1;32m     70\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     74\u001b[0m ]\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTreinando o modelo...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_for_pruning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemovendo wrappers de poda...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Remover wrappers de poda\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_callbacks.py:61\u001b[0m, in \u001b[0;36mUpdatePruningStep.on_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# If the model is newly created/initialized, set the 'pruning_step' to 0.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# If the model is saved and then restored, do nothing.\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprunable_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpruning_step\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     62\u001b[0m   tuples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprunable_layers:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PruneLowMagnitude' object has no attribute 'pruning_step'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel deu pane ao executar o código na célula atual ou em uma célula anterior. \n",
      "\u001b[1;31mAnalise o código nas células para identificar uma possível causa da pane. \n",
      "\u001b[1;31mClique <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqui</a> para obter mais informações. \n",
      "\u001b[1;31mConsulte Jupyter <a href='command:jupyter.viewOutput'>log</a> para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Configuração inicial\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "validation_split = 0.1\n",
    "num_images = x_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Simplificação do modelo\n",
    "def simplify_model(model):\n",
    "    simplified_model = tf.keras.Sequential()\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            simplified_model.add(tf.keras.layers.Conv2D(\n",
    "                filters=max(8, layer.filters // 2),\n",
    "                kernel_size=layer.kernel_size,\n",
    "                strides=layer.strides,\n",
    "                padding=layer.padding,\n",
    "                activation=layer.activation\n",
    "            ))\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            simplified_model.add(tf.keras.layers.Dense(\n",
    "                units=max(16, layer.units // 2),\n",
    "                activation=layer.activation\n",
    "            ))\n",
    "        else:\n",
    "            simplified_model.add(layer)\n",
    "    return simplified_model\n",
    "\n",
    "model = simplify_model(model)\n",
    "\n",
    "# Aplicar poda manualmente\n",
    "def apply_pruning_to_layers(model):\n",
    "    pruned_layers = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):\n",
    "            pruned_layers.append(tfmot.sparsity.keras.prune_low_magnitude(layer))\n",
    "        else:\n",
    "            pruned_layers.append(layer)\n",
    "    return tf.keras.Sequential(pruned_layers)\n",
    "\n",
    "model_for_pruning = apply_pruning_to_layers(model)\n",
    "\n",
    "print(\"Aplicando poda ao modelo...\")\n",
    "# Configuração de poda\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.5,\n",
    "        final_sparsity=0.9,\n",
    "        begin_step=0,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Compilando...\")\n",
    "# Compilar o modelo\n",
    "model_for_pruning.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Treinar o modelo com callbacks\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', patience=3, verbose=1, factor=0.5, min_lr=1e-5\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "]\n",
    "\n",
    "print(\"Treinando o modelo...\")\n",
    "history = model_for_pruning.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Removendo wrappers de poda...\")\n",
    "# Remover wrappers de poda\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "# Converter para TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "# Salvar modelo TFLite\n",
    "pruned_model_path = \"./optimized_prunned_model/\"\n",
    "if not os.path.exists(pruned_model_path):\n",
    "    os.makedirs(pruned_model_path)\n",
    "\n",
    "pruned_tflite_file = pruned_model_path + \"Optimized_Prunned_Model.tflite\"\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "    f.write(pruned_tflite_model)\n",
    "\n",
    "print(\"Modelo TFLite podado e otimizado salvo em:\", pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
