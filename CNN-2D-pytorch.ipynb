{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração do Ambiente\n",
    "\n",
    "Para configurar o ambiente necessário para este notebook, execute os seguintes comandos no terminal:\n",
    "\n",
    "```bash\n",
    "conda create -n pytorch_env python=3.9 matplotlib seaborn pandas scikit-learn -y\n",
    "conda activate pytorch_env\n",
    "pip install torch torchvision ipykernel\n",
    "python -m ipykernel install --user --name=pytorch_env --display-name \"Python (pytorch_env)\"\n",
    "```\n",
    "\n",
    "Após executar os comandos, selecione o kernel **Python (pytorch_env)** no menu **Kernel > Change Kernel** do Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# #definindo o path para executar no LOCAL\n",
    "path = \"/home/pedro/projetoDL/dataset/processado/\"\n",
    "log_path_tmp = \"/home/pedro/projetoDL/log/torch/\"\n",
    "# definindo o path para log com timestamp\n",
    "log_path = log_path_tmp + \"/exp_\" + timestamp + \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU está conectada\")\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"Nome da GPU: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"Não conectado a uma GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.load(path + 'Y_train_NewApproach_Injected_v2.npz')\n",
    "Y= Y.f.arr_0\n",
    "\n",
    "X = np.load(path + 'X_train_NewApproach_Injected_v2.npz')\n",
    "X = X.f.arr_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, average_precision_score\n",
    "from scipy.stats import ks_2samp\n",
    "#import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import interp\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def extract_final_losses(history):\n",
    "    \"\"\"Função para extrair o melhor loss de treino e validação.\n",
    "\n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "\n",
    "    Retorno:\n",
    "    Dicionário contendo o melhor loss de treino e de validação baseado\n",
    "    no menor loss de validação.\n",
    "    \"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    idx_min_val_loss = np.argmin(val_loss)\n",
    "    return {'train_loss': train_loss[idx_min_val_loss], 'val_loss': val_loss[idx_min_val_loss]}\n",
    "\n",
    "def plot_training_error_curves(history):\n",
    "    \"\"\"Função para plotar as curvas de erro do treinamento da rede neural.\n",
    "\n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "\n",
    "    Retorno:\n",
    "    A função gera o gráfico do treino da rede e retorna None.\n",
    "    \"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_loss, label='Train')\n",
    "    ax.plot(val_loss, label='Validation')\n",
    "    ax.set(title='Training and Validation Error Curves', xlabel='Epochs', ylabel='Loss (MSE)')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def compute_performance_metrics(y, y_pred_class, y_pred_scores=None):\n",
    "    accuracy = accuracy_score(y, y_pred_class)\n",
    "    recall = recall_score(y, y_pred_class)\n",
    "    precision = precision_score(y, y_pred_class)\n",
    "    f1 = f1_score(y, y_pred_class)\n",
    "    performance_metrics = (accuracy, recall, precision, f1)\n",
    "    if y_pred_scores is not None:\n",
    "        skplt.metrics.plot_ks_statistic(y, y_pred_scores)\n",
    "        plt.savefig(\"ks_plot.png\")\n",
    "        plt.show()\n",
    "        y_pred_scores = y_pred_scores[:, 1]\n",
    "        auroc = roc_auc_score(y, y_pred_scores)\n",
    "        aupr = average_precision_score(y, y_pred_scores)\n",
    "        performance_metrics = performance_metrics + (auroc, aupr)\n",
    "    return performance_metrics\n",
    "\n",
    "def print_metrics_summary(accuracy, recall, precision, f1, auroc=None, aupr=None):\n",
    "    print()\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Accuracy:\", value=accuracy))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Recall:\", value=recall))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Precision:\", value=precision))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"F1:\", value=f1))\n",
    "    if auroc is not None:\n",
    "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUROC:\", value=auroc))\n",
    "    if aupr is not None:\n",
    "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUPR:\", value=aupr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Garbage collector para liberar memória RAM.\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Configurações iniciais\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Normalização dos dados\n",
    "def normalize_data(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "# Inicializando pesos (Xavier/Glorot)\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Definindo a rede neural\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(64 * 11 * 29, 64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.bn1(torch.relu(self.conv1(x))))\n",
    "        x = self.pool2(self.bn2(torch.relu(self.conv2(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout1(torch.relu(self.fc1(x)))\n",
    "        x = self.dropout2(self.fc2(x))  # Sem Sigmoid; BCEWithLogitsLoss aplica internamente\n",
    "        return x\n",
    "\n",
    "# EarlyStopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Função para calcular métricas\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configurações do KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_no = 0\n",
    "for train_idx, val_idx in kf.split(X, Y):\n",
    "    fold_no += 1\n",
    "    # Criando o diretório para salvar os arquivos\n",
    "    save_path = os.path.join(log_path, f\"{fold_no}_fold\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Dados de treino e validação\n",
    "    print(\"Fold:\", fold_no)\n",
    "    x_train, x_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = Y[train_idx], Y[val_idx]\n",
    "\n",
    "    # Normalizando os dados\n",
    "    # print(\"Normalizando os dados...\")\n",
    "    # x_train = normalize_data(x_train)\n",
    "    # x_val = normalize_data(x_val)\n",
    "\n",
    "    # Convertendo para tensores do PyTorch\n",
    "    print(\"Convertendo para tensores do PyTorch...\")\n",
    "    x_train, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    x_val, y_val = torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    # Criando DataLoaders\n",
    "\n",
    "    print(\"Criando DataLoaders...\")\n",
    "    batch_size = 256\n",
    "    train_dataset = TensorDataset(x_train.unsqueeze(1), y_train)\n",
    "    val_dataset = TensorDataset(x_val.unsqueeze(1), y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Inicializando o modelo, loss e otimizador\n",
    "    print(\"Inicializando o modelo, loss e otimizador...\")\n",
    "    model = ConvNet().to(device)\n",
    "    model.apply(init_weights)  # Aplicando inicialização de pesos\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "    # Redução de LR e EarlyStopping\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-5, verbose=True)\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "\n",
    "    # Treinando o modelo\n",
    "    print(\"Treinando o modelo...\")\n",
    "    num_epochs = 30\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Loop de treinamento com tqdm\n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\")\n",
    "        for inputs, targets in train_loop:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Acurácia de treino\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct_train += (preds.squeeze() == targets).sum().item()\n",
    "            total_train += targets.size(0)\n",
    "\n",
    "            # Atualizando tqdm com a perda e o learning rate\n",
    "            train_loop.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validação\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        y_preds, y_trues = [], []\n",
    "\n",
    "        val_loop = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loop:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs.squeeze(), targets).item()\n",
    "\n",
    "                # Acurácia de validação\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct_val += (preds.squeeze() == targets).sum().item()\n",
    "                total_val += targets.size(0)\n",
    "\n",
    "                y_preds.extend(outputs.cpu().numpy())\n",
    "                y_trues.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - \"\n",
    "              f\"Train Acc: {train_accuracy:.4f} - Val Acc: {val_accuracy:.4f} - LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Atualizando scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Checando EarlyStopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Avaliação\n",
    "    y_pred_class = (np.array(y_preds) > 0.5).astype(int)\n",
    "    accuracy = compute_metrics(y_trues, y_pred_class)\n",
    "\n",
    "    # Salvando o modelo\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, f\"fold_{fold_no}_model.pth\"))\n",
    "\n",
    "    # Liberação de memória\n",
    "    del model, x_train, x_val, y_train, y_val, train_loader, val_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import datetime\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "# import gc\n",
    "\n",
    "# # Configurações iniciais\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# # Definindo a rede neural\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "#         self.bn1 = nn.BatchNorm2d(32)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.dropout1 = nn.Dropout(0.3)\n",
    "#         self.fc1 = nn.Linear(64 * 11 * 29, 64)\n",
    "#         self.dropout2 = nn.Dropout(0.3)\n",
    "#         self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool1(self.bn1(torch.relu(self.conv1(x))))\n",
    "#         x = self.pool2(self.bn2(torch.relu(self.conv2(x))))\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.dropout1(torch.relu(self.fc1(x)))\n",
    "#         x = self.dropout2(torch.sigmoid(self.fc2(x)))\n",
    "#         return x\n",
    "\n",
    "# # Função para calcular métricas\n",
    "# def compute_metrics(y_true, y_pred):\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "#     accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "#     return accuracy\n",
    "\n",
    "# # Configurações do KFold\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# fold_no = 0\n",
    "# for train_idx, val_idx in kf.split(X, Y):\n",
    "#     fold_no += 1\n",
    "#     # Criando o diretório para salvar os arquivos\n",
    "#     save_path = os.path.join(log_path, f\"{fold_no}_fold\")\n",
    "    \n",
    "#     os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "#     # Dados de treino e validação\n",
    "#     x_train, x_val = X[train_idx], X[val_idx]\n",
    "#     y_train, y_val = Y[train_idx], Y[val_idx]\n",
    "\n",
    "#     # Convertendo para tensores do PyTorch\n",
    "#     x_train, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "#     x_val, y_val = torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "#     # Criando DataLoaders\n",
    "#     batch_size = 128\n",
    "#     train_dataset = TensorDataset(x_train.unsqueeze(1), y_train)\n",
    "#     val_dataset = TensorDataset(x_val.unsqueeze(1), y_val)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # Inicializando o modelo, loss e otimizador\n",
    "#     model = ConvNet().to(device)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#     # Treinando o modelo\n",
    "#     num_epochs = 30\n",
    "#     train_losses, val_losses = [], []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         # Loop de treinamento com tqdm\n",
    "#         train_loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\")\n",
    "#         for inputs, targets in train_loop:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs.squeeze(), targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "#             train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "#         train_loss = running_loss / len(train_loader)\n",
    "#         train_losses.append(train_loss)\n",
    "\n",
    "#         # Validação\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         y_preds, y_trues = [], []\n",
    "\n",
    "#         val_loop = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\")\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, targets in val_loop:\n",
    "#                 inputs, targets = inputs.to(device), targets.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 val_loss += criterion(outputs.squeeze(), targets).item()\n",
    "#                 y_preds.extend(outputs.cpu().numpy())\n",
    "#                 y_trues.extend(targets.cpu().numpy())\n",
    "\n",
    "#         val_loss /= len(val_loader)\n",
    "#         val_losses.append(val_loss)\n",
    "\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "#     # Avaliação\n",
    "#     y_pred_class = (np.array(y_preds) > 0.5).astype(int)\n",
    "#     accuracy = compute_metrics(y_trues, y_pred_class)\n",
    "\n",
    "#     # Salvando o modelo\n",
    "#     torch.save(model.state_dict(), os.path.join(save_path, f\"fold_{fold_no}_model.pth\"))\n",
    "\n",
    "#     # Plotando as curvas de perda\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.plot(train_losses, label='Train Loss')\n",
    "#     plt.plot(val_losses, label='Validation Loss')\n",
    "#     plt.title('Loss per Epoch')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(os.path.join(save_path, \"loss_curve.png\"))\n",
    "#     plt.close()\n",
    "\n",
    "#     # Liberação de memória\n",
    "#     del model, x_train, x_val, y_train, y_val, train_loader, val_loader\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
